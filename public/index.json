[{"content":"ChatGPT is Bullshit The following is my reading and lecture notes on Michael Townsen Hicks\u0026rsquo;s philosophical article on the algorithm of Large Language Models and whether their algorithm\u0026rsquo;s logic is similar to \u0026ldquo;reasoning\u0026rdquo; or not. Hicks argues that LLM\u0026rsquo;s are bullshitters.\nReading Notes AI’s don’t lie or hallucinate, they bullshit. They don’t \u0026ldquo;care\u0026rdquo; about the truth one way other, they just make stuff up. That’s a problem because they’re programmed to appear to care about truthfulness, even they don’t have any real notion of what that is. They’ve been designed to mislead us. It is trained to form logical sentences. It isn’t trained to actually understand it’s output, limitation and such. Actually, they\u0026rsquo;re trained to form probable sentences. It\u0026rsquo;s only because we usually write logically that logical sentences are probable. The term \u0026ldquo;hallucinate\u0026rdquo; comes from vision model research, where a model is trained to identify a certain kind of thing, say faces, and then it identifies a \u0026ldquo;face\u0026rdquo; in a shadow pattern, or maybe light poking through the leaves of a tree. The AI is constructing signal from a set of inputs that don\u0026rsquo;t contain the thing it\u0026rsquo;s supposed to find. The term was adapted to language models to refer to an imprecise set of circumstances, such as factual incorrectness, fabricated information, task misalignment. The term \u0026lsquo;hallucinate\u0026rsquo;, however, doesn\u0026rsquo;t make much sense with respect to transformer-based generative models, because they always make up whatever they\u0026rsquo;re tasked to output. If you read generative AI papers from a decade ago (the DeepDream era), they will use \u0026ldquo;hallucination\u0026rdquo; to mean all output, not just the \u0026ldquo;lies\u0026rdquo;. Every ChatGPT responses is equally hallucinatory; some responses are just better at fooling users that they are drawing on \u0026ldquo;knowledge\u0026rdquo; whatsoever. \u0026ldquo;Bullshit\u0026rdquo; gets us closer because it centers the idea that the system is simply not concerned with the accuracy of its output at all.\nHallucinations aren’t some bug or error case, but merely the product of the exact same process that gave us accurate information. But the magic of generative AI is that so often that bullshit does align with the truth. That\u0026rsquo;s also exactly why they targeted visual arts so quickly, because it\u0026rsquo;s easier to hide flaws when so much of it is subjective. AI isn’t experiencing anything, so calling it hallucination didn’t make any sense to begin with. It’s like tech people just picked a random word out of a psychology textbook and were like, “yeah, let’s go with that.” There was an excellent guide to AI Microsoft put out that basically outlines this. They described it as AI “wants to please” which is why the WAY you ask it / prompt it matters. If your prompt has bias or assumptions baked into the question, AI tends to not want to contradict you. Source This has to do with the way word embeddings in LLMs “cluster” around semantic meanings, so when the AI attempts to retrieve a response it enters a vector space of words with similar semantic meaning for its “prediction” of the “correct response” the user wants. Possible Questions Would you say that \u0026ldquo;truthiness\u0026rdquo; is unique to ChatGPT, or does this concept apply to other mainstream media and information technologies as well? How does ChatGPT compare to those? We talked about how output is produced. But the illusion is also lies in the how it turns our input to an output. This gives the illusion of \u0026ldquo;understanding\u0026rdquo;. This was a very stimulating paper. It led me to dig deeper. I came across with an article on NYT, dated March 2023, co-written by Noam Chomsky. At the end of the article, they were describing moral stance/indifference of ChatGPT by borrowing a highly influential notion from Hannah Arendt, \u0026ldquo;the banality of evil\u0026rdquo;. So this is what they wrote on the article:\nChatGPT exhibits something like the banality of evil: plagiarism and apathy and obviation. It summarizes the standard arguments in the literature by a kind of super-autocomplete, refuses to take a stand on anything, pleads not merely ignorance but lack of intelligence and ultimately offers a “just following orders” defense, shifting responsibility to its creators.\nSo, your argument of \u0026ldquo;ChatGPT as a soft bullshitter\u0026rdquo; also has a related kind of indifference, but it is centered around truth rather than morality. So, there’s a moral difference between calling ChatGPT’s outputs “hallucinations” versus “bullshit,” but there\u0026rsquo;s also another step from “bullshit” to “banality of evil” as a concept. Arendt’s “banality of evil” goes further morally.\nI know these two are focusing two different problems: One is related to truth, the other is to morality. But still, I feel like they are connected. I am interested in hearing your thoughts on this. Do you think applying \u0026ldquo;the banality of evil\u0026rdquo; to LLM\u0026rsquo;s exaggerates their role, or does it complement to your \u0026ldquo;bullshitting\u0026rdquo; argument? Can Arendt\u0026rsquo;s concept is better to understand LLM\u0026rsquo;s relationship to truth and accountability than \u0026ldquo;bullshitting\u0026rdquo;, or do these ideas work best together?\nLecture Notes How do LLMs work? LLMs are based on an architecture of machine learning called a \u0026rsquo;transformer model\u0026rsquo; or \u0026lsquo;foundation model\u0026rsquo;. This is a form of machine learning. The model consists in a large number of connected probability functions. This is a Bayes net. These functions are fed a large amount of data typically text from the internet. They use that data to construct a predictive probabilities of the texts. What word tokens are likely to appear together? The model itself associates with each text with two vectors in high-dimensional spaces. The first locates a word (token) in a high-dimensional space near other words, that appear in similar contexts. We can think of this as representing the meaning of the word. However, it only has some of what we associate with meaning: the similarity between a word and other words, not necessarily word-world connections or inferential connections. The second locates the word\u0026rsquo;s surroundings in a high-dimensional space. We can think of this as representing the word\u0026rsquo;s context. However, it again only includes some of what we think of as context. The model selects randomly among the likely next words in the given context. Then they are trained with data. When it goes wrong? The probabilities that the model produces and uses do not represent the likelihood of proposition\u0026rsquo;s being true. They instead represent the likelihood of a word being used. This is correlated with truth: words are more likely to feature in true sonetences, provided the training data is mostly true. ","permalink":"http://localhost:1313/posts/first/","summary":"\u003ch1 id=\"chatgpt-is-bullshit\"\u003eChatGPT is Bullshit\u003c/h1\u003e\n\u003cblockquote class=\"note\"\u003e\u003cp\u003eThe following is my reading and lecture notes on Michael Townsen Hicks\u0026rsquo;s philosophical article on the algorithm of Large Language Models and whether their algorithm\u0026rsquo;s logic is similar to \u0026ldquo;reasoning\u0026rdquo; or not. Hicks argues that LLM\u0026rsquo;s are bullshitters.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"reading-notes\"\u003eReading Notes\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eAI’s don’t lie or hallucinate, they bullshit. They don’t \u0026ldquo;care\u0026rdquo; about the truth one way other, they just make stuff up.\n\u003cul\u003e\n\u003cli\u003eThat’s a problem because \u003cstrong\u003ethey’re programmed to appear to care about truthfulness, even they don’t have any real notion of what that is.\u003c/strong\u003e They’ve been designed to mislead us.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eIt is trained to form logical sentences. It isn’t trained to actually understand it’s output, limitation and such.\n\u003cul\u003e\n\u003cli\u003eActually, \u003cstrong\u003ethey\u0026rsquo;re trained to form probable sentences\u003c/strong\u003e. It\u0026rsquo;s only because we usually write logically that logical sentences are probable.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThe term \u003cstrong\u003e\u0026ldquo;hallucinate\u0026rdquo; comes from vision model research\u003c/strong\u003e, where a model is trained to identify a certain kind of thing, say faces, and then it identifies a \u0026ldquo;face\u0026rdquo; in a shadow pattern, or maybe light poking through the leaves of a tree. The AI is constructing signal from a set of inputs that don\u0026rsquo;t contain the thing it\u0026rsquo;s supposed to find.\n\u003cul\u003e\n\u003cli\u003eThe term was adapted to language models to refer to an imprecise set of circumstances, such as factual incorrectness, fabricated information, task misalignment. \u003cstrong\u003eThe term \u0026lsquo;hallucinate\u0026rsquo;, however, doesn\u0026rsquo;t make much sense with respect to transformer-based generative models, because they always make up whatever they\u0026rsquo;re tasked to output.\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eIf you read generative AI papers from a decade ago (the DeepDream era), they will use \u0026ldquo;hallucination\u0026rdquo; to mean all output, not just the \u0026ldquo;lies\u0026rdquo;.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eEvery\u003c/em\u003e ChatGPT responses is \u003cem\u003eequally\u003c/em\u003e hallucinatory; some responses are just better at fooling users that they are drawing on \u0026ldquo;knowledge\u0026rdquo; whatsoever.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u0026ldquo;Bullshit\u0026rdquo; gets us closer because it centers the idea that the system is simply not concerned with the accuracy of its output at all.\u003c/p\u003e","title":"The author is dead, long live the author!"},{"content":"Hi! My name is Berkant Gültekin and I\u0026rsquo;m an Analytics Engineer from Turkey. I\u0026rsquo;ve always liked learning new things and exploring new possibilities and this blog is an outlet for me to create and discover new things related to Data Science.\nI want to write posts that people can read without a technical background, but I also want to showcase and talk about the coding aspects of what I\u0026rsquo;m currently learning, so you\u0026rsquo;ll be able to find a little bit of everything here.\nYou can find me at:\nmedeiros-jaqueline\ndevmedeiros\ncurriculum vitae\nDisclaimer This is my personal website. The opinions expressed here are my own and do not reflect the views of my current or previous employers. If you would like to learn about the terms and policies, please visit this page.\n","permalink":"http://localhost:1313/about/","summary":"Berkant Gültekin, Analytical Engineer passionate about Data Science. In her blog she mixes the technical with the accessible, offering content about programming and discoveries for readers of different skill levels.","title":"about"}]