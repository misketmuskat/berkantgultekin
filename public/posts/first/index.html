<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ChatGPT is Bullshit | berkant gültekin</title>
<meta name=keywords content="artificial intelligence,llm,language"><meta name=description content="My reading and lecture notes on the same named article."><meta name=author content="berkant gültekin"><link rel=canonical href=http://localhost:1313/posts/first/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.a60f6677242bade864ed929dd3304cb40eaff2a29134f46d0092f392e0bb4c58.css integrity="sha256-pg9mdyQrrehk7ZKd0zBMtA6v8qKRNPRtAJLzkuC7TFg=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/first/><link rel=alternate hreflang=tr href=http://localhost:1313/tr/posts/first/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/first/"><meta property="og:site_name" content="berkant gültekin"><meta property="og:title" content="ChatGPT is Bullshit"><meta property="og:description" content="My reading and lecture notes on the same named article."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-27T01:14:44+01:00"><meta property="article:modified_time" content="2025-01-27T01:14:44+01:00"><meta property="article:tag" content="Artificial Intelligence"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Language"><meta property="og:image" content="http://localhost:1313/images/chatgpt.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/chatgpt.png"><meta name=twitter:title content="ChatGPT is Bullshit"><meta name=twitter:description content="My reading and lecture notes on the same named article."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"ChatGPT is Bullshit","item":"http://localhost:1313/posts/first/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ChatGPT is Bullshit","name":"ChatGPT is Bullshit","description":"My reading and lecture notes on the same named article.","keywords":["artificial intelligence","llm","language"],"articleBody":"ChatGPT is Bullshit The following is my reading and lecture notes on Michael Townsen Hicks’s philosophical article on the algorithm of Large Language Models and whether their algorithm’s logic is similar to “reasoning” or not. Hicks argues that LLM’s are bullshitters.\nReading Notes AI’s don’t lie or hallucinate, they bullshit. They don’t “care” about the truth one way other, they just make stuff up. That’s a problem because they’re programmed to appear to care about truthfulness, even they don’t have any real notion of what that is. They’ve been designed to mislead us. It is trained to form logical sentences. It isn’t trained to actually understand it’s output, limitation and such. Actually, they’re trained to form probable sentences. It’s only because we usually write logically that logical sentences are probable. The term “hallucinate” comes from vision model research, where a model is trained to identify a certain kind of thing, say faces, and then it identifies a “face” in a shadow pattern, or maybe light poking through the leaves of a tree. The AI is constructing signal from a set of inputs that don’t contain the thing it’s supposed to find. The term was adapted to language models to refer to an imprecise set of circumstances, such as factual incorrectness, fabricated information, task misalignment. The term ‘hallucinate’, however, doesn’t make much sense with respect to transformer-based generative models, because they always make up whatever they’re tasked to output. If you read generative AI papers from a decade ago (the DeepDream era), they will use “hallucination” to mean all output, not just the “lies”. Every ChatGPT responses is equally hallucinatory; some responses are just better at fooling users that they are drawing on “knowledge” whatsoever. “Bullshit” gets us closer because it centers the idea that the system is simply not concerned with the accuracy of its output at all.\nHallucinations aren’t some bug or error case, but merely the product of the exact same process that gave us accurate information. But the magic of generative AI is that so often that bullshit does align with the truth. That’s also exactly why they targeted visual arts so quickly, because it’s easier to hide flaws when so much of it is subjective. AI isn’t experiencing anything, so calling it hallucination didn’t make any sense to begin with. It’s like tech people just picked a random word out of a psychology textbook and were like, “yeah, let’s go with that.” There was an excellent guide to AI Microsoft put out that basically outlines this. They described it as AI “wants to please” which is why the WAY you ask it / prompt it matters. If your prompt has bias or assumptions baked into the question, AI tends to not want to contradict you. Source This has to do with the way word embeddings in LLMs “cluster” around semantic meanings, so when the AI attempts to retrieve a response it enters a vector space of words with similar semantic meaning for its “prediction” of the “correct response” the user wants. Possible Questions Would you say that “truthiness” is unique to ChatGPT, or does this concept apply to other mainstream media and information technologies as well? How does ChatGPT compare to those? We talked about how output is produced. But the illusion is also lies in the how it turns our input to an output. This gives the illusion of “understanding”. This was a very stimulating paper. It led me to dig deeper. I came across with an article on NYT, dated March 2023, co-written by Noam Chomsky. At the end of the article, they were describing moral stance/indifference of ChatGPT by borrowing a highly influential notion from Hannah Arendt, “the banality of evil”. So this is what they wrote on the article:\nChatGPT exhibits something like the banality of evil: plagiarism and apathy and obviation. It summarizes the standard arguments in the literature by a kind of super-autocomplete, refuses to take a stand on anything, pleads not merely ignorance but lack of intelligence and ultimately offers a “just following orders” defense, shifting responsibility to its creators.\nSo, your argument of “ChatGPT as a soft bullshitter” also has a related kind of indifference, but it is centered around truth rather than morality. So, there’s a moral difference between calling ChatGPT’s outputs “hallucinations” versus “bullshit,” but there’s also another step from “bullshit” to “banality of evil” as a concept. Arendt’s “banality of evil” goes further morally.\nI know these two are focusing two different problems: One is related to truth, the other is to morality. But still, I feel like they are connected. I am interested in hearing your thoughts on this. Do you think applying “the banality of evil” to LLM’s exaggerates their role, or does it complement to your “bullshitting” argument? Can Arendt’s concept is better to understand LLM’s relationship to truth and accountability than “bullshitting”, or do these ideas work best together?\nLecture Notes How do LLMs work? LLMs are based on an architecture of machine learning called a ’transformer model’ or ‘foundation model’. This is a form of machine learning. The model consists in a large number of connected probability functions. This is a Bayes net. These functions are fed a large amount of data typically text from the internet. They use that data to construct a predictive probabilities of the texts. What word tokens are likely to appear together? The model itself associates with each text with two vectors in high-dimensional spaces. The first locates a word (token) in a high-dimensional space near other words, that appear in similar contexts. We can think of this as representing the meaning of the word. However, it only has some of what we associate with meaning: the similarity between a word and other words, not necessarily word-world connections or inferential connections. The second locates the word’s surroundings in a high-dimensional space. We can think of this as representing the word’s context. However, it again only includes some of what we think of as context. The model selects randomly among the likely next words in the given context. Then they are trained with data. When it goes wrong? The probabilities that the model produces and uses do not represent the likelihood of proposition’s being true. They instead represent the likelihood of a word being used. This is correlated with truth: words are more likely to feature in true sonetences, provided the training data is mostly true. ","wordCount":"1070","inLanguage":"en","image":"http://localhost:1313/images/chatgpt.png","datePublished":"2025-01-27T01:14:44+01:00","dateModified":"2025-01-27T01:14:44+01:00","author":{"@type":"Person","name":"berkant gültekin"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/first/"},"publisher":{"@type":"Organization","name":"berkant gültekin","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="berkant gültekin (Alt + H)">berkant gültekin</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=http://localhost:1313/tr/ title=tr aria-label=tr>tr</a></li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/archives title=archive><span>archive</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">ChatGPT is Bullshit</h1><div class=post-description>My reading and lecture notes on the same named article.</div><div class=post-meta><span title='2025-01-27 01:14:44 +0100 CET'>January 27, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;berkant gültekin&nbsp;|&nbsp;read in:<ul class=i18n_list><li><a href=http://localhost:1313/tr/posts/first/>tr</a></li></ul></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#reading-notes>Reading Notes</a></li><li><a href=#possible-questions>Possible Questions</a></li><li><a href=#lecture-notes>Lecture Notes</a></li></ul></nav></div></details></div><div class=post-content><h1 id=chatgpt-is-bullshit>ChatGPT is Bullshit<a hidden class=anchor aria-hidden=true href=#chatgpt-is-bullshit>#</a></h1><blockquote class=note><p>The following is my reading and lecture notes on Michael Townsen Hicks&rsquo;s philosophical article on the algorithm of Large Language Models and whether their algorithm&rsquo;s logic is similar to &ldquo;reasoning&rdquo; or not. Hicks argues that LLM&rsquo;s are bullshitters.</p></blockquote><h2 id=reading-notes>Reading Notes<a hidden class=anchor aria-hidden=true href=#reading-notes>#</a></h2><ul><li>AI’s don’t lie or hallucinate, they bullshit. They don’t &ldquo;care&rdquo; about the truth one way other, they just make stuff up.<ul><li>That’s a problem because <strong>they’re programmed to appear to care about truthfulness, even they don’t have any real notion of what that is.</strong> They’ve been designed to mislead us.</li></ul></li><li>It is trained to form logical sentences. It isn’t trained to actually understand it’s output, limitation and such.<ul><li>Actually, <strong>they&rsquo;re trained to form probable sentences</strong>. It&rsquo;s only because we usually write logically that logical sentences are probable.</li></ul></li><li>The term <strong>&ldquo;hallucinate&rdquo; comes from vision model research</strong>, where a model is trained to identify a certain kind of thing, say faces, and then it identifies a &ldquo;face&rdquo; in a shadow pattern, or maybe light poking through the leaves of a tree. The AI is constructing signal from a set of inputs that don&rsquo;t contain the thing it&rsquo;s supposed to find.<ul><li>The term was adapted to language models to refer to an imprecise set of circumstances, such as factual incorrectness, fabricated information, task misalignment. <strong>The term &lsquo;hallucinate&rsquo;, however, doesn&rsquo;t make much sense with respect to transformer-based generative models, because they always make up whatever they&rsquo;re tasked to output.</strong></li><li>If you read generative AI papers from a decade ago (the DeepDream era), they will use &ldquo;hallucination&rdquo; to mean all output, not just the &ldquo;lies&rdquo;.</li><li><em>Every</em> ChatGPT responses is <em>equally</em> hallucinatory; some responses are just better at fooling users that they are drawing on &ldquo;knowledge&rdquo; whatsoever.</li></ul></li></ul><p>&ldquo;Bullshit&rdquo; gets us closer because it centers the idea that the system is simply not concerned with the accuracy of its output at all.</p><ul><li>Hallucinations <strong>aren’t some bug or error case,</strong> but merely <strong>the product of the exact same process that gave us accurate information</strong>. But the magic of generative AI is that so often that bullshit does align with the truth.<ul><li>That&rsquo;s also exactly why they targeted visual arts so quickly, because it&rsquo;s easier to hide flaws when so much of it is subjective.</li></ul></li><li>AI isn’t experiencing anything, so calling it hallucination didn’t make any sense to begin with. It’s like tech people just picked a random word out of a psychology textbook and were like, “yeah, let’s go with that.”</li><li>There was an excellent guide to AI Microsoft put out that basically outlines this. They described it as AI “wants to please” which is why the WAY you ask it / prompt it matters. If your prompt has bias or assumptions baked into the question, AI tends to not want to contradict you. <a href=https://www.microsoft.com/en-us/security/blog/2024/06/04/ai-jailbreaks-what-they-are-and-how-they-can-be-mitigated/>Source</a><ul><li>This has to do with the way <strong>word embeddings in LLMs “cluster” around semantic meanings</strong>, so when the AI attempts to retrieve a response it enters a vector space of words with similar semantic meaning for its “prediction” of the “correct response” the user wants.</li></ul></li></ul><h2 id=possible-questions>Possible Questions<a hidden class=anchor aria-hidden=true href=#possible-questions>#</a></h2><ul><li>Would you say that &ldquo;truthiness&rdquo; is unique to ChatGPT, or does this concept apply to other mainstream media and information technologies as well? How does ChatGPT compare to those?</li><li>We talked about how output is produced. But the illusion is also lies in the how it turns our input to an output. This gives the illusion of &ldquo;understanding&rdquo;.</li></ul><p>This was a very stimulating paper. It led me to dig deeper. I came across with an article on NYT, dated March 2023, co-written by Noam Chomsky. At the end of the article, they were describing moral stance/indifference of ChatGPT by borrowing a highly influential notion from Hannah Arendt, &ldquo;the banality of evil&rdquo;. So this is what they wrote on the article:</p><blockquote><p>ChatGPT exhibits something like the banality of evil: plagiarism and apathy and obviation. It summarizes the standard arguments in the literature by a kind of super-autocomplete, refuses to take a stand on anything, pleads not merely ignorance but lack of intelligence and ultimately offers a “just following orders” defense, shifting responsibility to its creators.</p></blockquote><p>So, your argument of &ldquo;ChatGPT as a soft bullshitter&rdquo; also has a related kind of indifference, but it is centered around truth rather than morality. So, there’s a moral difference between calling ChatGPT’s outputs “hallucinations” versus “bullshit,” but there&rsquo;s also another step from “bullshit” to “banality of evil” as a concept. Arendt’s “banality of evil” goes further morally.</p><p>I know these two are focusing two different problems: One is related to truth, the other is to morality. But still, I feel like they are connected.
I am interested in hearing your thoughts on this. Do you think applying &ldquo;the banality of evil&rdquo; to LLM&rsquo;s exaggerates their role, or does it complement to your &ldquo;bullshitting&rdquo; argument? Can Arendt&rsquo;s concept is better to understand LLM&rsquo;s relationship to truth and accountability than &ldquo;bullshitting&rdquo;, or do these ideas work best together?</p><h2 id=lecture-notes>Lecture Notes<a hidden class=anchor aria-hidden=true href=#lecture-notes>#</a></h2><ul><li><strong>How do LLMs work?</strong><ul><li>LLMs are based on an architecture of machine learning called a &rsquo;transformer model&rsquo; or &lsquo;foundation model&rsquo;.</li><li>This is a form of machine learning. The model consists in a large number of connected probability functions.<ul><li>This is a Bayes net.</li></ul></li><li>These functions are fed a large amount of data typically text from the internet.</li><li>They use that data to construct a predictive probabilities of the texts. What word tokens are likely to appear together?</li><li>The model itself associates with each text with <strong>two vectors</strong> in high-dimensional spaces.</li><li>The first locates a word (token) in a high-dimensional space near other words, that appear in similar contexts.<ul><li>We can think of this as representing the meaning of the word.</li><li>However, it only has <strong>some</strong> of what we associate with meaning: the similarity between a word and other words, not necessarily word-world connections or inferential connections.</li></ul></li><li>The second locates the word&rsquo;s surroundings in a high-dimensional space.<ul><li>We can think of this as representing the word&rsquo;s context.</li><li>However, it again only includes some of what we think of as context.</li></ul></li><li>The model <strong>selects randomly</strong> among the likely next words in the given context.</li></ul></li><li><strong>Then they are trained with data.</strong></li><li><strong>When it goes wrong?</strong><ul><li>The probabilities that the model produces and uses <strong>do not represent</strong> the likelihood of proposition&rsquo;s being true.</li><li>They instead represent the likelihood of a word being used.</li><li>This is correlated with truth: words are more likely to feature in true sonetences, provided the training data is mostly true.</li><li></li></ul></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/artificial-intelligence/>Artificial Intelligence</a></li><li><a href=http://localhost:1313/tags/llm/>Llm</a></li><li><a href=http://localhost:1313/tags/language/>Language</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share ChatGPT is Bullshit on x" href="https://x.com/intent/tweet/?text=ChatGPT%20is%20Bullshit&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2ffirst%2f&amp;hashtags=artificialintelligence%2cllm%2clanguage"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ChatGPT is Bullshit on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2ffirst%2f&amp;title=ChatGPT%20is%20Bullshit&amp;summary=ChatGPT%20is%20Bullshit&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2ffirst%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ChatGPT is Bullshit on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2ffirst%2f&title=ChatGPT%20is%20Bullshit"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ChatGPT is Bullshit on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2ffirst%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ChatGPT is Bullshit on whatsapp" href="https://api.whatsapp.com/send?text=ChatGPT%20is%20Bullshit%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2ffirst%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share ChatGPT is Bullshit on telegram" href="https://telegram.me/share/url?text=ChatGPT%20is%20Bullshit&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2ffirst%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>berkant gültekin</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>